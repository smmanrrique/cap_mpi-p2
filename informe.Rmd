---
title: "Práctica 2: Programación de Comunicaciones en MPI"
author: "Shamuel Manrrique 802400 \\n
         Aldrix Marfil 794976"
date: "08/01/2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

# Resumen

En esta práctica se abordan algunos patrones de comunicación clásicos en paso de
mensajes. La idea es caracterizar el comportamiento de dichos patrones y para ello 
se realizaron dos actividades con estos patrones:

  1. La primera tarea consistió en la implementación de estos patrones en lenguaje
  C utilizando MPI. 
  
  2. La segunda tarea consistió en la caracterización del comportamiento de estos
  patrones de comunicación.

## Actividad 1: Implementación en C de comunicaciones en MPI  

Para lograr los requerimientos de esta práctica se realizaron tres implementaciones:

  - Prueba de latencia: 
  Para esta sección se implementó el script **latency_prueba.c** el cual es un programa 
  en MPI que mide la latencia de las comunicaciones por cada pareja asignadas a jugar 
  ping-pong(rebotar paquetes). La cantidad de envíos (rebotes) que se realizan se 
  encuentra parametrizados.
  
  - Prueba de ancho de banda: 
  Para esta sección se implementó el script **bandwidth_prueba.c** el cual es un programa 
  en MPI que mide el ancho de bandas de un envío de N paquetes (a modo de ráfaga) de 
  tamaño M entre dos procesos, un emisor y un receptor que emite un ACK de validación 
  una vez recibido los N paquetes. 
  
  - Prueba de latencia de la operación de broadcast: 
  Para esta sección se implementó el script **broadcast_prueba.c** el cual es un programa 
  en MPI que tiene dos funcionalidades:
  
    1. Broadcast One-to-one:  
    Envio de mensajes usando Send/Receive en donde un proceso es el root y tiene que 
    hacer el envió uno a uno al resto de procesos.
    
    2. Broadcast Bcast: Envío usando la función de Bcast de MPI el cual realiza el 
    envío simultáneo a todos los procesos, solo se le indica el proceso raíz en la 
    misma llamada.

## Actividad 2: Caracterización de patrones

Caracterizar los patrones requiere obtener medidas de sus comportamientos. Para ello se 
empleó la función MPI_Wtime con la intención de calcular las siguientes medidas:
  - Tiempos de ejecución, 
  - Velocidades de transferencia
  - Latencias de comunicación
  
A la hora de medir los tiempos se realizaron experimentos ejecutando muchas repeticiones 
para minimizar la influencia de valores atípicos y de la falta de resolución del medidor 
de tiempo en las medidas.

### Pruebas realizadas
  Se realizaron las siguientes pruebas para cada uno de las implementaciones descritas en 
  la actividad 1:

  - Prueba de latencia: 
    * Tiempo de rebote: 
        Por cada pareja asignada se midió el tiempo de un rebote. Esto es, por ejemplo, 
        para la pareja 0 - 1, se midió el tiempo que tarda el paquete en ser enviado desde 
        0 a 1 mas el tiempo del reenvío que hace 1 a 0. 
    * Tiempo total de rebote:
        Como el numero de rebotes puede variar, se midió el timepo total que tardan todos
        los rebotes en ejecutarse por proceso. 

  - Prueba de ancho de banda: 
    * Tiempo de envío de paquetes:
        Por cada par de parejas se midió el tiempo de recepción de una ráfaga de mensajes. 
        Esto es, por ejemplo, para la pareja 0 - 1, se midió el tiempo que tarda el proceso
        0 en enviar una cantidad N de paquetes y luego 1 envia un paquete confirmacion al 
        proceso 0 y se detiene la medición. 
    * Ancho de banda:
        Teniendo la cantidad de paquetes enviados (N), el tamaño de su tipo de datos (T) y 
        el tiempo en segundos (E) que tarda el envío de la ráfaga hasta su confirmacion se 
        puede calcular el ancho de banda como el maximo ancho de banda obtenido de sacar la 
        siguiente cuenta para todos las rafagas de las parejas: Bd = (N*T) / E  
    * Velocidad de transferencia:
        Esta velocidad de transferencia efectiva vendria dada por la fórmula anterior pero
        tomando el promedio de todas las cuentas Bd = (N*T) / E 
  
  - Prueba de latencia de la operación de broadcast: 
      Se fijó el proceso 0 como root (es quien envia los paquetes a los otros nodos). Luego, 
      se midió el tiempo que tardan los envios del proceso root a los otros procesos para 
      las siguientes dos implementaciones:
        1. Broadcast One-to-one  
        2. Broadcast Bcast

### Análisis de resultados de rendimiento

### Importar librerías necesarias
```{r setup, warning=FALSE,message=FALSE}
library("ggplot2")
library("dplyr")         # load
library("RcmdrMisc")
library("nleqslv")       # Resolver sistema ecuaciones lineales/no lineales
library("readr")
library("sqldf")
path = "C:/Users/smmanrrique/3D Objects/unizar/cap/cap_mpi-p2/results/"
```

### Prueba de latencia entre procesos en MPI

A continuación se muestran medidas sobre el tiempo de rebote de un paquete entre 
dos nodos con la finalidad de apreciar la latencia existente:

El eje X indica la cantidad de procesos usados en la prueba y el eje Y representa el 
tiempo de rebote (en segundos).

```{r, warning=FALSE,message=FALSE}
# File name
archivo = "latency_1000pkt.csv"
read_csv = paste(path,archivo , sep="")

# Read and import csv
latency <- read.csv("C:/Users/smmanrrique/3D Objects/unizar/cap/cap_mpi-p2/results/latency_1000pkt.csv")

latency = filter(latency, TYPE==" RS" )

#------------------------------------------------------------------------------#
# Resumen estadisticos
numSummary(latency[,c("COM_TIME"), drop=FALSE], groups = latency$NPROC, statistics=c("mean", "sd", "IQR", "quantiles", "skewness", "kurtosis"), quantiles=c(0,.25,.5,.75,1))

# Grafica Boxplot general
Boxplot(COM_TIME~NPROC, data=latency, id=list(method="y"),outline=FALSE, xlab="Number of Process", ylab="Time")

x4 = filter(latency, NPROC==4)
x8 = filter(latency, NPROC==8)
x16 = filter(latency, NPROC==16)

# Prueba de normalidad por grupo
normalityTest(x4$COM_TIME, test="ad.test")   # Anderson-Darling
normalityTest(x8$COM_TIME, test="ad.test")   # Anderson-Darling
normalityTest(x16$COM_TIME, test="ad.test")   # Anderson-Darling

# Test de varianza
var.test(x4$COM_TIME,x8$COM_TIME)
var.test(x16$COM_TIME,x8$COM_TIME)


# Validamos si las dos medias son iguales o no 
# Ejecucion en un nodo y dos nodos distintos 
wilcox.test( x4$COM_TIME, x8$COM_TIME, mu= 0,paired = FALSE, alternative = "two.sided", conf.int = T)
# Ejecucion en nodos distintos 
wilcox.test( x16$COM_TIME, x8$COM_TIME, mu= 0,paired = FALSE, alternative = "two.sided", conf.int = T)


resumen_latency = sqldf("SELECT  COUNT(DISTINCT(NODE)) MACHINES, NPROC PROCESS, SUM(PACKET_SIZE) TAMAÑO_TOTAL,MIN(COM_TIME) WORST_TIME, MAX(COM_TIME) BEST_TIME, AVG(COM_TIME) AVERAGE FROM latency GROUP BY  NPROC")
resumen_latency

#------------------------------------------------------------------------------#




```


Para esta prueba se toma en cuenta la ejecución del mismo número de paquetes con tamaño 500(OJO) con distintos número de procesos 4 (única máquina) y 8-16(varias máquinas) para comparar los tiempos entre ellos. Se puede observar en el resumen estadístico que los datos están esparcidos por lo que para la gráfica de boxplot se omiten valores atípicos se procede a corroborar que existe una diferencia entre la ejecución del programa con todos los slots en una máquina o en distintas máquinas. Para poder comprobar si existe diferencia entre los grupos primero se realizó un test de normalidad donde se comprobó que ninguno de los datos sigue una distribución normal. También se validó que la varianza de los grupos son distintas. Realizada estas pruebas se empleó la prueba de wilcoxon donde se observa que el valor p se encuentra debajo de nuestro nivel de significancia ( α=0,05), con lo cual rechazamos la hipótesis nula(los grupos poseen la misma media) y concluimos que hay una diferencia estadísticamente significativa entre las dos medicamentos.

Por ende se puede concluir que la comunicación entre procesos usando MPI tiene un peso proporcional al número de máquinas distintas involucradas en el cálculo.  


### Prueba de Bandwidth entre procesos en MPI 
```{r}
# File name
archivo = "bandwidth_1000pkt.csv"
read_csv = paste(path,archivo , sep="")

# Read and import csv
bandwidth <- read.csv(read_csv, header=T, dec='.', sep=',', na.strings = "")

bandwidth= filter(latency, TYPE==" RS")
#------------------------------------------------------------------------------#
# Resumen estadisticos de los datos
numSummary(bandwidth[,c("COM_TIME"), drop=FALSE], groups = bandwidth$NPROC, statistics=c("mean", "sd", "IQR", "quantiles", "skewness", "kurtosis"), quantiles=c(0,.25,.5,.75,1))

# Grafica Boxplot general
Boxplot(COM_TIME~NPROC, data=bandwidth, id=list(method="y"),outline=FALSE, xlab="Number of Process", ylab="Time")

sqldf("SELECT  COUNT(DISTINCT(NODE)) MACHINES, NPROC PROCESS, MIN(COM_TIME) WORST_TIME, MAX(COM_TIME) BEST_TIME, AVG(COM_TIME) AVERAGE FROM bandwidth GROUP BY  NPROC")

#------------------------------------------------------------------------------#
```

### Prueba de Broadcast entre procesos en MPI 
```{r}
# File name
archivo = "broadcast_1000pkt.csv"
read_csv = paste(path,archivo , sep="")

# Read and import csv
broadcast <- read.csv(read_csv, header=T, dec='.', sep=',', na.strings = "")
#------------------------------------------------------------------------------#
broadcast_0 = filter(broadcast, BCAST_TYPE==0)

numSummary(broadcast_0[,c("COM_TIME"), drop=FALSE], groups = broadcast_0$NPROC, statistics=c("mean", "sd", "IQR", "quantiles", "skewness", "kurtosis"), quantiles=c(0,.25,.5,.75,1))

# Grafía de Boxplot de Homicidios por regiones
Boxplot(COM_TIME~NPROC, data=broadcast_0, id=list(method="y"), xlab="Number of Process", ylab="Time")

broadcast_1 = filter(broadcast, BCAST_TYPE==1)

numSummary(broadcast_1[,c("COM_TIME"), drop=FALSE], groups = broadcast_1$NPROC, statistics=c("mean", "sd", "IQR", "quantiles", "skewness", "kurtosis"), quantiles=c(0,.25,.5,.75,1))

# Grafía de Boxplot de Homicidios por regiones
Boxplot(COM_TIME~NPROC, data=broadcast_1, id=list(method="y"), xlab="Number of Process", ylab="Time")


sqldf("SELECT BCAST_TYPE, COUNT(DISTINCT(NODE)) MACHINES, NPROC PROCESS, MIN(COM_TIME) WORST_TIME, MAX(COM_TIME) BEST_TIME, AVG(COM_TIME) AVERAGE FROM broadcast GROUP BY BCAST_TYPE, NPROC")
#------------------------------------------------------------------------------#
```
Se puede concluir que usando la librería Bcast el tiempo se reduce a la mitad en comparación de hacer el envió uno a uno a cada uno de los procesos desde un nodo raíz.

# Apendice: 

## Archivos de ejecución 

Para la ejecución de forma automatizada y parametrizada de cada uno de los script de C se crearon 
bash scripts para automatizar los experimentos. Un ejemplo de los siguientes valores especificados 
para la ejecución son:  
```
PROGRAM="name.out"         -> Nombre del archivo compilado con mpicc.
CSV_NAME="name_prueba"     -> Nombre del archivo csv con los resultados.
HOSTFILE="host_name.txt"   -> Nombre del Host File a usar
PACKET_SIZES=(500)         -> Arreglo con los distintos tamaños del Paquete
NUMBER_PACKETS=(1000)      -> Arreglo con los distintos número de prueba/ejecuciones.
NUMBER_PROCCESS=(4 8 16)   -> Arreglo con los distintos número de procesos.
```

## Archivo resultante de las ejecuciones
Para  facilitar el análisis de los resultados obtenidos de tiempo de ejecución, velocidad de transferencia 
y latencia de comunicación por cada uno de los scripts se guarda un archivo de salida .csv con la
siguiente cabecera:
```
BCAST_TYPE   -> Tipo de broadcast en caso de aplicar (solo aplica a los pruebas de broadcast).
PACKET_SIZE  -> Tamaño en Bytes del paquete.
NPROC        -> Cantidad de procesos.
N_PACKETS    -> Cantidad de paquetes a enviar.
N_BOUNCES    -> Número de rebotes del paquete de un proceso a otro (solo aplica al prueba de latencia).
NODE         -> Máquina donde se ejecuta el proceso.
PROCESS      -> Identificador(Rank) del proceso.
SRC          -> Proceso que envía el/los mensaje/s
DST          -> Proceso que recibe el/los mensaje/s
TAG          -> Identificador único de parejas conectadas.
COM_TIME     -> Uso MPI_Wtime para medir el tiempo de envío y confirmación de recepción.
RUNING_TIME  -> Tiempo de ejecución del proceso.
```

Por mantener consistencia en todos los .csv dependiendo del script que se ejecute puede mostrar alguna 
columna vacía dado que ese dato no es requerido para ese script en particular. Por otra parte el PACKET_SIZE 
como se está usando tipo entero (4 Bytes) se multiplica el tamaño introducido por cuatro bytes para obtener 
la cantidad total en bytes, las medidas de tiempo usadas COM_TIME y RUNING_TIME estan expresadas en segundos.


