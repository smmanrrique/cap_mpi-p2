---
title: "Práctica 2: Programación de Comunicaciones en MPI"
author: "Shamuel Manrrique 802400 \\n
         Aldrix Marfil 794976"
date: "08/01/2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# Resumen

En esta práctica se abordan algunos patrones de comunicación clásicos en paso de
mensajes. La idea es caracterizar el comportamiento de dichos patrones y para ello 
se realizaron las siguientes dos actividades:

  1. La primera tarea consistió en la implementación de estos patrones en lenguaje
  C utilizando MPI. 
  
  2. La segunda tarea consistió en la caracterización del comportamiento de estos
  patrones de comunicación.

## Actividad 1: Implementación en C de comunicaciones en MPI  

Para lograr los requerimientos de esta práctica se realizaron tres implementaciones:

  - **Prueba de latencia:**
  Para esta sección se implementó el script **latency_test.c** el cual es un programa 
  en MPI que mide la latencia de las comunicaciones por cada pareja asignadas a jugar 
  Ping-pong (rebotar paquetes). La cantidad de envíos (rebotes) que se realizan se 
  encuentra parametrizados.
  
  - **Prueba de ancho de banda:**
  Para esta sección se implementó el script **bandwidth_test.c** el cual es un programa 
  en MPI que mide el ancho de banda de un envío de N paquetes (a modo de ráfaga) de 
  tamaño M entre dos procesos, un emisor y un receptor que emite un ACK de validación 
  una vez recibido los N paquetes de la ráfaga. 
  
  - **Prueba de latencia de la operación de broadcast:** 
  Para esta sección se implementó el script **broadcast_test.c** el cual es un programa 
  en MPI que tiene dos funcionalidades:
  
    1. **Broadcast One-to-one:**  
    Envío de mensajes usando Send/Receive en donde un proceso es el root y tiene que 
    hacer el envió uno a uno al resto de procesos.
    
    2. **Broadcast Bcast:** Envío usando la función de Bcast de MPI el cual realiza el 
    envío simultáneo a todos los procesos, solo se le indica el proceso raíz en la 
    misma llamada.

## Actividad 2: Caracterización de patrones

Caracterizar los patrones requiere obtener medidas de sus comportamientos. Para ello se calcularon las siguientes medidas:
- Tiempos de ejecución
- Velocidades de transferencia
- Latencias de comunicación
  
A la hora de medir los tiempos se realizaron experimentos ejecutando varias repeticiones (ejecutar cada prueba varias veces) para minimizar la influencia de valores atípicos y de la 
falta de resolución del medidor de tiempo en las medidas.

### Pruebas realizadas

Se realizaron diversas pruebas para experimentar distintos escenarios y observar cómo varían  
las distintas medidas de interés. Para ello, se utilizaron dos computadoras conectadas, cada una con 4 núcleos disponibles. Las pruebas realizadas involucran ejecuciones usando 4 procesos (mapeados en una misma máquina) y 8 procesos (mapeados en ambas máquinas). Respecto al tamaño de los paquetes que se envían los procesos tenemos 1 Byte, 1 Kilobyte, 1 Megabyte, 10 Megabyte. 

Finalmente se calcularon las siguientes medidas para cada una de las implementaciones descritas en la actividad 1 y con los valores de parámetros mencionados previamente:

  - **Prueba de latencia:** 
    * **Tiempo de rebote:** 
        Por cada pareja asignada se midió el tiempo de un rebote. Esto es, por ejemplo, 
        para la pareja 0 - 1, se midió el tiempo que tarda el paquete en ser enviado desde 
        0 a 1 mas el tiempo del envío que tarda de 1 a 0. 
    * **Tiempo total de rebote:**
        Como el número de rebotes puede variar, se midió el tiempo total que tardan todos
        los rebotes en ejecutarse por cada par de procesos. 

  - **Prueba de ancho de banda:** 
    * **Tiempo de envío de paquetes:**
        Por cada par de parejas se midió el tiempo de recepción de una ráfaga de mensajes. 
        Esto es, por ejemplo, para la pareja 0 - 1, se midió el tiempo que tarda el proceso
        0 en enviar una cantidad N de paquetes y luego 1 envía un paquete confirmación al 
        proceso 0 y se detiene la medición. 
    * **Ancho de banda:**
        Teniendo la cantidad de paquetes enviados (N), el tamaño de su tipo de datos (T) y 
        el tiempo en segundos (E) que tarda el envío de la ráfaga hasta su confirmación se 
        puede calcular el ancho de banda como el máximo ancho de banda obtenido de sacar la 
        siguiente cuenta para todos las rafagas de las parejas: Bd = (N*T) / E  
    * **Velocidad de transferencia:**
        Esta velocidad de transferencia efectiva vendría dada por la fórmula anterior pero
        tomando el promedio de todas las cuentas Bd = (N*T) / E 
  
  - **Prueba de latencia de la operación de broadcast:**
      Se fijó el proceso 0 como root (es quien envía los paquetes a los otros nodos). Luego, 
      se midió el tiempo que tardan los envíos del proceso root a los otros procesos para 
      las siguientes dos implementaciones:
1. Broadcast One-to-one  
2. Broadcast Bcast

### Análisis de resultados de rendimiento

<!-- ### Importar librerías necesarias -->
```{r setup, warning=FALSE,message=FALSE, echo=FALSE}
library("ggplot2")
library("dplyr")         # load
library("RcmdrMisc")
library("nleqslv")       # Resolver sistema ecuaciones lineales/no lineales
library("readr")
library("sqldf")
path = "C:/Users/smmanrrique/3D Objects/unizar/cap/cap_mpi-p2/results/"
```

### Prueba de latencia entre procesos en MPI

Se realizaron 10 ejecuciones del script de latencia. Y se emplearon variaciones de los parámetros Número de procesos (4 y 8), Tamaño del paquete enviado (1B, 10 KB, 1 MB, 10MB). Es importante recordar que acá se mide la cantidad de tiempo que tarda un paquete en rebotar entre un nodo A y un nodo B. 

A continuación se mostrará el resumen estadístico de las latencias obtenidas para experimentos usando 4 y 8 procesos con envíos de paquetes de 1 Byte y 10 Megabytes. 

```{r, warning=FALSE,message=FALSE, echo=FALSE}
# Read and import csv
latency_exp <- read.csv("C:/Users/smmanrrique/3D Objects/unizar/cap/cap_mpi-p2/results/latency_experimento_10.csv")
latency_exp = filter(latency_exp, TYPE==" RS")

# Resumen estadísticos con PACKET_SIZE==1 por proceso
numSummary(filter(latency_exp, PACKET_SIZE==1)[,c("COM_TIME"), drop=FALSE], groups = filter(latency_exp, PACKET_SIZE==1)$NPROC, statistics=c("mean", "sd", "IQR", "quantiles", "skewness", "kurtosis"), quantiles=c(0,.25,.5,.75,1))

# Resumen estadísticos con PACKET_SIZE==10000000 por proceso
numSummary(filter(latency_exp, PACKET_SIZE==10000000)[,c("COM_TIME"), drop=FALSE], groups = filter(latency_exp, PACKET_SIZE==10000000)$NPROC, statistics=c("mean", "sd", "IQR", "quantiles", "skewness", "kurtosis"), quantiles=c(0,.25,.5,.75,1))
```
Se puede determinar por los resultados que el tiempo de envío y recepción del paquete depende del tamaño del paquete, a mayor tamaño mayor latencia. También se puede deducir que el número de procesos mapeados en distintas máquinas usadas para la ejecución influyen en el tiempo.  

Para visualizar mejor la diferencia de latencia entre usar 4 y 8 procesos mapeados en distintas máquinas se muestra un diagrama de caja donde se aprecia el número de procesos en relación al tiempo de latencia. A partir de 4 procesos el mapeo de procesos se lleva a cabo entre dos máquinas. Por lo que el coste de comunicación es mucho mayor, Los procesos quedan distribuidos de forma tal que las parejas están separadas en máquinas distintas.

#### Gráfico de caja del tiempo promedio por número de proceso 
```{r, warning=FALSE,message=FALSE, echo=FALSE}
latency = latency_exp

# Grafica Boxplot general
box_latency = Boxplot(COM_TIME~NPROC, data=filter(latency_exp, PACKET_SIZE==10000000), id=list(method="y"),outline=FALSE, xlab="Number of Process", ylab="Time")

# Resumen estadísticos
x4 = filter(latency, NPROC==4)
x8 = filter(latency, NPROC==8)

# Prueba de normalidad por grupo
n4 = normalityTest(x4$COM_TIME, test="ad.test")   # Anderson-Darling
n8 = normalityTest(x8$COM_TIME, test="ad.test")   # Anderson-Darling

# Test de varianza
v48 = var.test(x4$COM_TIME,x8$COM_TIME)


# Validamos si las dos medias son iguales o no 
# Ejecución en un nodo y dos nodos distintos 
wc = wilcox.test( x4$COM_TIME, x8$COM_TIME, mu= 0,paired = FALSE, alternative = "two.sided", conf.int = T)
```

Se observó que los datos están esparcidos y para la gráfica de boxplot se omiten valores atípicos para tener una visualización más legible y se procede a corroborar que existe una diferencia entre la ejecución del programa con todos los slots en una máquina o en distintas máquinas. Para poder comprobar si existe diferencia entre los grupos primero se realizó un test de normalidad donde se comprobó que ninguno de los datos sigue una distribución normal. También se validó que la varianza de los grupos son distintas. Realizada estas pruebas se empleó la prueba de wilcoxon donde se observa que el valor p se encuentra debajo de nuestro nivel de significancia ( α=0,05), con lo cual rechazamos la hipótesis nula (los grupos poseen la misma media) y concluimos que hay una diferencia estadísticamente significativa entre las dos medias.

Por lo tanto se puede concluir que la comunicación entre procesos usando MPI tiene un peso proporcional al número de máquinas distintas involucradas en el cálculo. 

#### Métricas de latencia promediando los experimentos

A continuación se presenta una tabla resumen de los tiempos por tamaño del paquete y número de procesos
```{r, echo=FALSE}
sqldf("SELECT   PACKET_SIZE, NPROC, ROUND(MIN(RUNNING_TIME),4) BEST_TIME, ROUND(MAX(RUNNING_TIME),4) WORST_TIME, ROUND(AVG(RUNNING_TIME),4) AVERAGE  FROM latency_exp  WHERE RUNNING_TIME IS NOT NULL GROUP BY NPROC,PACKET_SIZE  ")
# message(sprintf("Message="))
```
Observación: Estos resultados toman en cuenta el mejor y peor de los tiempos de entre todos los 10 experimentos realizados.

#### Tabla resumen del bandwidth y throughput

Ahora se muestra una tabla con el máximo y mínimo tiempo que toman los rebotes. Tambíen se muestra el máximo ancho de banda en cada caso (bandwidth) y la velocidad promedio de transferencia (throughput). 

```{r, echo=FALSE}
resumen_latency_exp = sqldf("SELECT   PACKET_SIZE, NPROC, MIN(COM_TIME) BEST_TIME, MAX(COM_TIME) WORST_TIME, AVG(COM_TIME) AVERAGE, ROUND((PACKET_SIZE/MIN(COM_TIME))/1048576,6) BANDWIDTH, ROUND((PACKET_SIZE/AVG(COM_TIME))/1048576, 4) THROUGHPUT  FROM latency_exp  GROUP BY NPROC,PACKET_SIZE ")

resumen_latency_exp
```

#### Gráfica comparativa de tiempo de envío por tamaño del paquete distinguiendo por proceso 

Se aplicó la función logaritmo a los ejes valores de la gráfica en ambos ejes para poder apreciar mejor la forma de la curva ya que los valores son muy pequeños.

```{r, echo=FALSE}
ggplot(resumen_latency_exp, aes(x=PACKET_SIZE, y=log(AVERAGE), group = NPROC, colour =NPROC )) + 
  geom_line()  + 
  geom_point( size=2, shape=21, fill="white") + 
  labs(y = 'Tiempo promedio de envío',
          x = 'Tamaño del paquete')+
  theme_minimal()
```

Con las pruebas realizadas se puede apreciar que tanto el tamaño del paquete y el número de máquinas en la que se ejecutan los procesos influyen en  los resultados obtenidos. El tiempo incrementa casi al doble al incluir nuevas máquinas. Se puede observar también que a partir de cierto umbral en el caso de 4 procesos  PACKET_SIZE=1000000 (1MB) y en 8 procesos PACKET_SIZE=10000 (10KB) el bandwidth llega a su máxima capacidad y luego disminuye, esto se puede deber a que se requiere hacer más particiones del paquete para su envío. 


### Prueba de Bandwidth entre procesos en MPI 
En esta oportunidad se realizaron 10 ejecuciones del script de bandwidth. Y se emplearon variaciones de los parámetros Número de procesos (4 y 8), Tamaño del paquete enviado (1B, 10 KB, 1 MB, 10MB). Es importante recordar que acá se mide la cantidad de tiempo que tarda una ráfaga de 100 paquetes en ser enviado desde un nodo A a un nodo B. 

A continuación se mostrará el resumen estadístico de los tiempos obtenidos para experimentos usando 4 y 8 procesos con envíos de paquetes de 1 Byte y 10 Megabytes. 

```{r, echo=FALSE}
# Read and import csv
bandwidth_exp <- read.csv("C:/Users/smmanrrique/3D Objects/unizar/cap/cap_mpi-p2/results/bandwidth_experiment_10.csv")
bandwidth_exp = filter(bandwidth_exp, TYPE==" SR" )

bandwidth_exp4 = filter(bandwidth_exp, NPROC==4)
# Resumen estadísticos con PACKET_SIZE==1 por proceso
numSummary(bandwidth_exp4[,c("COM_TIME"), drop=FALSE], groups = bandwidth_exp4$PACKET_SIZE, statistics=c("mean", "sd", "IQR", "quantiles", "skewness", "kurtosis"), quantiles=c(0,.25,.5,.75,1))
```


```{r, echo=FALSE}
bandwidth_exp8 = filter(bandwidth_exp, NPROC==8)

# Resumen estadísticos con PACKET_SIZE==1 por proceso
numSummary(bandwidth_exp8[,c("COM_TIME"), drop=FALSE], groups = bandwidth_exp8$PACKET_SIZE, statistics=c("mean", "sd", "IQR", "quantiles", "skewness", "kurtosis"), quantiles=c(0,.25,.5,.75,1))
```

Se puede determinar por los resultados que el tiempo de envío y recepción de la rafaga de paquetes depende del tamaño del paquete, a mayor tamaño mayor tiempo. También el número de procesos mapeados en distintas máquinas usadas para la ejecución influyen en el tiempo.  

Para visualizar mejor la diferencia de latencia entre usar 4 y 8 procesos mapeados en distintas máquinas se muestra un diagrama de caja donde se aprecia el número de procesos en relación al tiempo en recibir la rafaga y confirmarla.

```{r, warning=FALSE,message=FALSE, echo=FALSE}
bandwidth = bandwidth_exp
temp =filter(bandwidth_exp, PACKET_SIZE==10000000)

# Grafica Boxplot general
box_bandwidth = Boxplot(COM_TIME~NPROC, data=temp, id=list(method="y"),outline=FALSE, xlab="Number of Process", ylab="Time")
```

Se observa que existe una gran diferencia de tiempo respecto a la cantidad de procesos usados cuando están mapeados en distintas máquinas puesto que en la rafaga de paquetes se envía bastantes datos comparado a la versión anterior de un paquete rebotando entre dos procesos.

#### Tabla comparativa de los resultados obtenidos para bandwidth

Ahora se muestra una tabla con el máximo y mínimo tiempo que toma enviar la ráfaga, recibir todos los paquetes y confirmar al emisor. Tambíen se muestra el máximo ancho de banda en cada caso (bandwidth) y la velocidad promedio de transferencia (throughput). 

```{r, echo=FALSE}
resumen_bandwidth_exp = sqldf("SELECT   PACKET_SIZE, NPROC, MIN(COM_TIME) BEST_TIME, MAX(COM_TIME) WORST_TIME, AVG(COM_TIME) AVERAGE, ROUND((PACKET_SIZE/MIN(COM_TIME))/1048576,6) BANDWIDTH, ROUND((PACKET_SIZE/AVG(COM_TIME))/1048576, 4) THROUGHPUT  FROM bandwidth  GROUP BY NPROC,PACKET_SIZE ")

resumen_bandwidth_exp
```

#### Gráfica comparativa de los tiempos por número de procesos

Se aplicó la función logaritmo a los ejes valores de la gráfica en ambos ejes para poder apreciar mejor la forma de la curva ya que los valores son muy pequeños.

```{r, warning=FALSE,message=FALSE, echo=FALSE}

ggplot(resumen_bandwidth_exp, aes(x=PACKET_SIZE, y=log(AVERAGE), group = NPROC, colour =NPROC )) + 
  geom_line()  + 
  geom_point( size=2, shape=21, fill="white") + 
  labs(y = 'Tiempo promedio de envío',
          x = 'Tamaño del paquete')+
  theme_minimal()
```
 
Se puede observar que el comportamiento de latencia respecto a cuando se envía una gran rafaga de paquetes (100 en total) es similar al de los rebotes. 


### Prueba de Broadcast entre procesos en MPI 

### Envíos one to one
#### Resumen estadístico (4 procesos)

Se muestra una tabla resumen con los tiempos obtenidos realizando 10  pruebas con 4 procesos donde el nodo root asignado es el cero y era el encargado de enviar mensajes a todos los demás procesos.

```{r, echo=FALSE}
# Read and import csv
broadcast <- read.csv("C:/Users/smmanrrique/3D Objects/unizar/cap/cap_mpi-p2/results/broadcast_exp_new_10.csv")

# Envío de paquetes usando de Bcast
broadcast_0 = filter(broadcast, BCAST_TYPE==0, TYPE == "S")

bcast_4 = filter(broadcast_0, NPROC == 4)
numSummary(bcast_4[,c("COM_TIME"), drop=FALSE], groups = bcast_4$PACKET_SIZE, statistics=c("mean", "sd", "IQR", "quantiles", "skewness", "kurtosis"), quantiles=c(0,.25,.5,.75,1))
```
En resumen estadístico de la ejecución con 4 procesos se puede observar que el tiempo medio va incrementando con el tamaño del paquete, la desviación estándar se mantiene  bastante alta. El valor del skewness nos indica que la distribución de la muestra es asimétrica positiva y tiene datos atípicos.

#### Resumen estadístico de haciendo envíos uno a uno (8 procesos)
```{r, echo=FALSE}
bcast_8 = filter(broadcast_0, NPROC == 8)
numSummary(bcast_8[,c("COM_TIME"), drop=FALSE], groups = bcast_8$PACKET_SIZE, statistics=c("mean", "sd", "IQR", "quantiles", "skewness", "kurtosis"), quantiles=c(0,.25,.5,.75,1))
```

En resumen estadístico de la ejecución con 8 procesos vemos el mismo incremento de tiempos del caso anterior. Se puede resaltar que existe una menor desviación y no se posee tantos datos atípicos, sin embargo el tiempo  de comunicación es mayor.


#### Gráfico de caja del tiempo promedio por número de procesos (PACKET_SIZE == 10000000)
```{r, warning=FALSE,message=FALSE, echo=FALSE}
size5 = filter(broadcast_0, PACKET_SIZE == 10000000 )
box_broadcast1 = Boxplot(COM_TIME~NPROC, data=broadcast_0, id=list(method="y"), xlab="Number of Process", ylab="Time")
```

Se puede apreciar el número de máquinas en los tiempos de comunicación  al incrementar  el número de procesos y máquinas involucradas..



#### Tabla resumen
```{r, echo=FALSE}
resumen_broadcast_exp = sqldf("SELECT  BCAST_TYPE, PACKET_SIZE, NPROC, MIN(COM_TIME) BEST_TIME, MAX(COM_TIME) WORST_TIME, AVG(COM_TIME) AVERAGE, ROUND((PACKET_SIZE/MIN(COM_TIME))/1048576, 4) BANDWIDTH, ROUND((PACKET_SIZE/AVG(COM_TIME))/1048576, 4) THROUGHPUT  FROM broadcast_0 WHERE BCAST_TYPE = 0 GROUP BY BCAST_TYPE, NPROC,PACKET_SIZE ")

resumen_broadcast_exp
```

Además de la influencia del número de máquinas en el tiempo se puede apreciar también que tanto el bandwidth como el throughput se ve afectado considerablemente por la comunicación remota.  


#### Gráfica comparativa de tiempo de envío por tamaño del paquete distinguiendo por proceso 
```{r, echo=FALSE}
ggplot(resumen_broadcast_exp, aes(x=PACKET_SIZE, y=log(AVERAGE), group = NPROC, colour =NPROC )) + 
  geom_line()  + 
  geom_point( size=2, shape=21, fill="white") + 
  labs(y = 'Tiempo promedio de envío',
       x = 'Tamaño del paquete')+
  theme_minimal()
```
La gráfica muestra la diferencia en tiempo de ejecutar el mismo programa con paquetes de distintos tamaños entre usando una cantidad variable de procesos.  

### Envio Bcast
#### Resumen estadístico de envíos usando la función Bcast de MPI (para 4 procesos)
```{r, warning=FALSE,message=FALSE, echo=FALSE}
# Tomas los datos 
broadcast_1 = filter(broadcast, BCAST_TYPE==1)

bcast_1_4 = filter(broadcast_1, NPROC == 4)
numSummary(bcast_1_4[,c("COM_TIME"), drop=FALSE], groups = bcast_1_4$PACKET_SIZE, statistics=c("mean", "sd", "IQR", "quantiles", "skewness", "kurtosis"), quantiles=c(0,.25,.5,.75,1))
```
Se observa cómo se reduce a más de la mitad el tiempo promedio de envío usando la librería Bcast, el tamaño del paquete influye también en el tiempo de los envíos. Se puede concluir que usando la librería Bcast el tiempo se reduce a la mitad en comparación a enviar uno a uno a cada uno de los procesos desde un nodo raíz.

#### Resumen estadístico de envíos usando usando la función Bcast de MPI  (para 8 procesos)
```{r, warning=FALSE,message=FALSE, echo=FALSE}
bcast_1_8 = filter(broadcast_1, NPROC == 8)
numSummary(bcast_1_8[,c("COM_TIME"), drop=FALSE], groups = bcast_1_8$PACKET_SIZE, statistics=c("mean", "sd", "IQR", "quantiles", "skewness", "kurtosis"), quantiles=c(0,.25,.5,.75,1))
```

#### Gráfico de caja del tiempo promedio de envío por número de procesos usando Bcast (tamaño de paquete 10MB)

```{r, warning=FALSE,message=FALSE, echo=FALSE}
size5 = filter(broadcast_1, PACKET_SIZE == 10000000 )

box_broadcast1 = Boxplot(COM_TIME~NPROC, data=broadcast_1, id=list(method="y"), xlab="Number of Process", ylab="Time")

```


Según las mediciones obtenidas no hay una diferencia significativa en usar la función Bcast MPI o el envío uno a uno, al menos para menos de 8 procesos. Se aprecia en el siguiente gráfico de caja como las medias de las mediciones para cada grupo (4 y 8 procesos) son similares.

#### Tabla resumen (Tiempos y Datos transmitidos)
```{r, echo=FALSE}
resumen_broadcast_exp = sqldf("SELECT  BCAST_TYPE, PACKET_SIZE, NPROC, MIN(COM_TIME) BEST_TIME, MAX(COM_TIME) WORST_TIME, AVG(COM_TIME) AVERAGE, ROUND((PACKET_SIZE/MIN(COM_TIME))/1048576, 4) BANDWIDTH, ROUND((PACKET_SIZE/AVG(COM_TIME))/1048576, 4) THROUGHPUT  FROM broadcast_1 GROUP BY BCAST_TYPE, NPROC,PACKET_SIZE ")

resumen_broadcast_exp
```

Se observan diferencias en tiempo en el orden de las diezmilésimas (cuarto decimal) para los mejores tiempos en los diferentes tamaños de paquetes. Luego, el máximo ancho de banda obtenido fue de 1875,47 MB por segundo con una transferencia promedio de datos de 1728.57. 

Es importante resaltar que el ancho de banda se ve influenciado por la cantidad de procesos usados, puesto que mientras más procesos se usen va disminuyendo el ancho de banda disponible.



#### Gráfica comparativa de tiempo de envío por tamaño del paquete distinguiendo por proceso 
```{r, echo=FALSE}
ggplot(resumen_broadcast_exp, aes(x=PACKET_SIZE, y=log(AVERAGE), group = NPROC, colour =NPROC )) + 
  geom_line()  + 
  geom_point( size=2, shape=21, fill="white") + 
  labs(y = 'Tiempo promedio de envío',
       x = 'Tamaño del paquete')+
  theme_minimal()
```

### Comparativa

Por los resultados obtenidos con los tiempos de (Bcast 1, one-to-one 0) y graficando para una mejor visualización de las diferencias.

##### Comparativa bcast vs one to one (4 procesos)

```{r, echo=FALSE}
b4 = filter(broadcast, NPROC == 4)
resumen_broadcast_exp = sqldf("SELECT  BCAST_TYPE, PACKET_SIZE, NPROC, MIN(COM_TIME) BEST_TIME, MAX(COM_TIME) WORST_TIME, AVG(COM_TIME) AVERAGE, ROUND((PACKET_SIZE/MIN(COM_TIME))/1048576, 4) BANDWIDTH, ROUND((PACKET_SIZE/AVG(COM_TIME))/1048576, 4) THROUGHPUT  FROM b4 GROUP BY BCAST_TYPE, NPROC,PACKET_SIZE ")

ggplot(resumen_broadcast_exp, aes(x=PACKET_SIZE, y=log(AVERAGE), group = BCAST_TYPE, colour =BCAST_TYPE )) + 
  geom_line()  + 
  geom_point( size=2, shape=21, fill="white") + 
  labs(y = 'Tiempo promedio de envío',
       x = 'Tamaño del paquete')+
  theme_minimal()
```

Respecto a los tiempos de no se aprecia una mayor diferencia con cuatro procesos entre ambas métodos. Por los resultados obtenidos en particular no correspondian con lo esperado dado que se tendería creer que la implementación de MPI de Bcast debe ser más eficiente en estos casos, pero los resultados obtenidos  nos dicen todo lo contrario.


##### Comparativa bcast vs one to one (8 procesos)

```{r, echo=FALSE}
b8 = filter(broadcast, NPROC == 8)
resumen_broadcast_exp = sqldf("SELECT  BCAST_TYPE, PACKET_SIZE, NPROC, MIN(COM_TIME) BEST_TIME, MAX(COM_TIME) WORST_TIME, AVG(COM_TIME) AVERAGE, ROUND((PACKET_SIZE/MIN(COM_TIME))/1048576, 4) BANDWIDTH, ROUND((PACKET_SIZE/AVG(COM_TIME))/1048576, 4) THROUGHPUT  FROM b8 GROUP BY BCAST_TYPE, NPROC,PACKET_SIZE ")

ggplot(resumen_broadcast_exp, aes(x=PACKET_SIZE, y=log(AVERAGE), group = BCAST_TYPE, colour =BCAST_TYPE )) + 
  geom_line()  + 
  geom_point( size=2, shape=21, fill="white") + 
  labs(y = 'Tiempo promedio de envío',
       x = 'Tamaño del paquete')+
  theme_minimal()
```

El desempeño de la versión MPI de broadcast es bastante similar al del envío uno a uno empleando varios procesos siempre y cuando el tamaño de los paquetes sea pequeño. Por ejemplo,  en la gráfica se observa que para un paquete de 10 MB si hay una brecha en la duración del envío a todos vs el envío uno a uno a favor del Bcast de MPI. Posiblemente, esta función de MPI está pensada para sacar mayor partido al ejecutar el programa usando un mayor número  de procesos.

# Apendice: 
## Archivos de ejecución 

Para la ejecución de forma automatizada y parametrizada de cada uno de los script de C se crearon 
bash scripts para automatizar los experimentos. Un ejemplo de los siguientes valores especificados 
para la ejecución son:  
```
PROGRAM="name.out"         -> Nombre del archivo compilado con mpicc.
CSV_NAME="name_prueba"     -> Nombre del archivo csv con los resultados.
HOSTFILE="host_name.txt"   -> Nombre del Host File a usar
PACKET_SIZES=(500)         -> Arreglo con los distintos tamaños del Paquete
NUMBER_PACKETS=(1000)      -> Arreglo con los distintos número de prueba/ejecuciones.
NUMBER_PROCCESS=(4 8 16)   -> Arreglo con los distintos número de procesos.
```

## Archivo resultante de las ejecuciones
Para  facilitar el análisis de los resultados obtenidos de tiempo de ejecución, velocidad de transferencia 
y latencia de comunicación por cada uno de los scripts se guarda un archivo de salida .csv con la
siguiente cabecera:
```
BCAST_TYPE   -> Tipo de broadcast en caso de aplicar (solo aplica a los pruebas de broadcast).
PACKET_SIZE  -> Tamaño en Bytes del paquete.
NPROC        -> Cantidad de procesos.
N_PACKETS    -> Cantidad de paquetes a enviar.
N_BOUNCES    -> Número de rebotes del paquete de un proceso a otro (solo aplica para la prueba de latencia).
NODE         -> Máquina donde se ejecuta el proceso.
PROCESS      -> Identificador(Rank) del proceso.
SRC          -> Proceso que envía el/los mensaje/s
DST          -> Proceso que recibe el/los mensaje/s
TAG          -> Identificador único de parejas conectadas.
COM_TIME     -> Uso MPI_Wtime para medir el tiempo de envío y confirmación de recepción.
RUNING_TIME  -> Tiempo de ejecución del proceso.
```

Por mantener consistencia en todos los .csv dependiendo del script que se ejecute puede mostrar alguna 
columna vacía dado que ese dato no es requerido para ese script en particular. Por otra parte el PACKET_SIZE 
como se está usando tipo entero (4 Bytes) se multiplica el tamaño introducido por cuatro bytes para obtener 
la cantidad total en bytes, las medidas de tiempo usadas COM_TIME y RUNING_TIME están expresadas en segundos.

---




